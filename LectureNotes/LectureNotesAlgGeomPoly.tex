\documentclass[11pt]{report}
\usepackage[a4paper,left=2.5cm,right=1.5cm,top=3cm,bottom=2cm]{geometry}
%\usepackage{t1enc,times,amsmath}
\usepackage{t1enc,times,amsmath,amsthm,amssymb} %,amsfonts}
\usepackage{hyperref}
\usepackage{graphicx}
%\usepackage[english]{babel}
%\usepackage[T1]{fontenc}
%\selectlanguage{english}
%\usepackage[swedish]{babel}
%\selectlanguage{swedish}

% For harvard style author year references.
%\usepackage{natbib}

\newcommand{\sinc}{\operatorname{sinc}}
\newcommand{\tr}{\operatorname{tr}}
\newcommand{\sz}{Szelisky}
\newcommand{\fl}{Finn Lindgren}
\newcommand{\fatR}{\mathbb{R}}
\newcommand{\fatC}{\mathbb{C}}
\newcommand{\fatZ}{\mathbb{Z}}
\newcommand{\fatN}{\mathbb{N}}
\newcommand{\rank}{\operatorname{rank}}
\newcommand{\KA}{K\AA}
\newcommand{\ie}{i.e.\ }
\newcommand{\eg}{e.g.\ }
\newcommand{\vc}[1]{{\bar{\mathbf{#1}}}}
\newcommand{\spc}{{\quad}}
\newcommand{\realR}{\mathbb{R}}
\newcommand{\complexC}{\mathbb{C}}
\newcommand{\bx}{{\bf x}}
\newcommand{\by}{{\bf y}}
\providecommand{\norm}[1]{\lVert#1\rVert}
\newcommand{\rrr}{\mathbf{r}}
\newcommand{\sss}{\mathbf{s}}


\newtheorem{theorem}{Theorem}[section]
\newtheorem{problem}{Problem}[section]
\newtheorem{conjecture}{Conjecture}[section]
% \newcommand{\refname}{\large References}
%\renewcommand{\qedsymbol}{\hfill \rule{2mm}{2mm}}
%\newcommand{\qedsymbol}{\hfill \rule{2mm}{2mm}}
%{\theoremstyle{definition}\newtheorem{definition}{Definition}[section]}
%{\theoremstyle{definition}\newtheorem{example}{Example}[section]}
\newtheorem{definition}{Definition}[section]
\newtheorem{example}{Example}[section]
\newtheorem{proposition}{Proposition}[theorem]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{corollary}{Corollary}[section]
\newenvironment{remark}{\noindent {\bf Remark.}\/}{\hfill \rule{2mm}{2mm}}
\newtheorem{formulation}{Formulation}
\newtheorem{algoritm}{Algorithm}[section]

\begin{document}
\centerline{{\huge \bf Lecture Notes for Algebraic Geometry and }}
\centerline{{\huge \bf Solving Systems of Polynomial Systems, 2018}}
\vspace{5mm}
\parindent 0pt

This document contains study notes for the course in 
Algebraic Geometry and Solving Systems of Polynomial Systems given at Lund University, 2018.
The notes are still work in progress, so read with caution. Errors and suggestions of improving the notes
are much appreciated. Just send me an e-mail and I'll try to fix it. 


%\cite{ahrnbom-jensen-etal-iccvw-17}

All material is posted on the course homepage: \\
\href{http://www.ctr.maths.lu.se/course/AlgGeomPoly/2018/}{http://www.ctr.maths.lu.se/course/AlgGeomPoly/2018/} \\

In the course we will study the theory of algebraic geometry and apply the theory to solving systems of polynomial equations.

For the course we have used several textbooks, \eg 


{\bf Ideals, Variations and Algorithms} by D. Cox, J. Little, D. O'Shea, Springer. The scope of the course is Chapters 1-4, and parts of Chapters 5 and 8.

{\bf Using Algebraic Geometry} by D. Cox, J. Little, D. O'Shea, Springer. The scope of the course is Chapter 2, as well as parts of Chapters 3 and 7. \\


{\bf Scientific papers}, e.g.\\

Larsson, V., Åström, K., \& Oskarsson, M. (2017). 
\href{http://openaccess.thecvf.com/content_ICCV_2017/papers/Larsson_Polynomial_Solvers_for_ICCV_2017_paper.pdf}{\bf Polynomial Solvers for Saturated Ideals}
In The IEEE International Conference on Computer Vision (ICCV) IEEE--Institute of Electrical and Electronics Engineers Inc.. 

Larsson, V., Åström, K., \& Oskarsson, M. (2017). 
\href{http://openaccess.thecvf.com/content_cvpr_2017/papers/Larsson_Efficient_Solvers_for_CVPR_2017_paper.pdf}{\bf Efficient Solvers for Minimal Problems by Syzygy-based Reduction}
In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017 

Larsson, V., \& Åström, K. (2016). 
\href{http://www.maths.lth.se/matematiklth/personal/viktorl/papers/larsson2016uncovering.pdf}{\bf Uncovering symmetries in polynomial systems}.
In B. Leibe, J. Matas, N. Sebe, \& M. Welling (Eds.), Computer Vision - ECCV 2016 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part III (pp. 252-267). (Lecture Notes in Computer Science (LNCS); Vol. 9907). Springer Verlag. 



\chapter{Examples of systems of polynomial equations}

\section{Converting a geometric problem to systems of polynomial equations}

Converting a geometric problem to systems of polynomial equations.


\begin{example}[Find a line that goes through one point]
Consider a point $(x,y)$ in a plane. what lines pass through this point. What lines goes through this point. Write this as a system of polynomial equations. What is the solution set, the variety? {\bf later:} What is the degree of variety? What is the dimension of the variety?
\end{example}

\begin{example}[Find a line that goes through two points]
Consider two point $(x_1,y_1), (x_2,y_2)$ in a plane. What lines pass through these two point? Write this as a system of polynomial equations. What is the solution set, the variety in the general case? Are there critical/exceptional data $(x_1,y_1), (x_2,y_2)$ so that this variety becomes larger? {\bf later:} What is the degree of variety? What is the dimension of the variety?
\end{example}

\begin{example}[Find a line that goes through two points]
Consider two point $(x_1,y_1), (x_2,y_2), (x_3,y_3)$ in a plane. What lines pass through these two point? Write this as a system of polynomial equations. What is the solution set, the variety in the general case? Are there critical/exceptional data $(x_1,y_1), (x_2,y_2), (x_3,y_3)$ so that this variety becomes larger? {\bf later:} What is the degree of variety? What is the dimension of the variety?
\end{example}

\begin{example}[Linear algebra]
Consider a matrix  
$$
A = \begin{pmatrix} a_1 & a_4 & a_5 \\
a_2 & 0 & 0 \\
a_3 & 0 & 0 \end{pmatrix}$$
and a vector
$$
b = \begin{pmatrix} b_1 \\ b_2 \\ b_3 \end{pmatrix}$$
The linear system of equations $Ax = b$ defines a variety? Is this problem typically overdetermined, underdetermined or well-determined? Could it be said to be both under and overdetermined?
\end{example}

\begin{example}[Determining the homography from four points]
Consider four points in one image $(x_1,y_1), (x_2,y_2), (x_3,y_3), (x_4,y_4)$ and
four points in another images $(u_1,v_1), (u_2,v_2), (u_3,v_3), (u_4,v_4)$. Determine the  $3\times 3$ matrix $H$ representing the homography, such that
$$ \lambda_i \begin{pmatrix} u_i \\ v_i \\ 1 \end{pmatrix} = H \begin{pmatrix} x_i \\ y_i \\ 1 \end{pmatrix} \forall i 
$$
What is the variety in this case? Dimension? Degree? Well-determined?
\end{example}

\begin{example}[Determining the homography from three points and one line]
Consider four points in one image $(x_1,y_1), (x_2,y_2), (x_3,y_3), (x_4,y_4)$ and
four points in another images $(u_1,v_1), (u_2,v_2), (u_3,v_3), (u_4,v_4)$. Determine the  $3\times 3$ matrix $H$ representing the homography, such that
$$ \lambda_i \begin{pmatrix} u_i \\ v_i \\ 1 \end{pmatrix} = H \begin{pmatrix} x_i \\ y_i \\ 1 \end{pmatrix} \forall i 
$$
What is the variety in this case? Dimension? Degree? Well-determined?
\end{example}

\begin{example}[Determining the homography from two points and two lines]
Consider four points in one image $(x_1,y_1), (x_2,y_2), (x_3,y_3), (x_4,y_4)$ and
four points in another images $(u_1,v_1), (u_2,v_2), (u_3,v_3), (u_4,v_4)$. Determine the  $3\times 3$ matrix $H$ representing the homography, such that
$$ \lambda_i \begin{pmatrix} u_i \\ v_i \\ 1 \end{pmatrix} = H \begin{pmatrix} x_i \\ y_i \\ 1 \end{pmatrix} \forall i 
$$
What is the variety in this case? Dimension? Degree? Well-determined?
\end{example}

\section{Suggested project problems}


\begin{example}[Uncalibrated relative pose for two views of seven points]
Consider seven points in one image $(x_1,y_1), (x_2,y_2), \ldots, (x_7,y_7)$ and
four points in another images $(u_1,v_1), (u_2,v_2), \ldots (u_7,v_7)$. Determine the  $3\times 3$ matrix $F$ representing the fundamental matrix, such that
$$ \lambda_i \begin{pmatrix} x_i \\ y_i \\ 1 \end{pmatrix}^T F \begin{pmatrix} u_i \\ v_i \\ 1 \end{pmatrix} = 0, \quad \forall i 
$$
and such that $$ \det(F) = 0 . $$
\end{example}

\begin{example}[Calibrated relative pose for two views of five points]
Consider five points in one image $(x_1,y_1), (x_2,y_2), \ldots, (x_5,y_5)$ and
four points in another images $(u_1,v_1), (u_2,v_2), \ldots (u_5,v_5)$. Determine the  $3\times 3$ matrix $E$ representing the essential matrix, such that
$$ \lambda_i \begin{pmatrix} x_i \\ y_i \\ 1 \end{pmatrix}^T F \begin{pmatrix} u_i \\ v_i \\ 1 \end{pmatrix} = 0, \quad \forall i 
$$
and such that $$ \det(F) = 0$$
and
$$ 2 E E^T E - \tr (EE^T) E = 0 . $$
\end{example}

\begin{example}[Planar trilateration]
Time-of-Arrival measurements gives (after multiplication with the speed of sound or speed of light) distance measurements. 
Assume that there are a set of $m=2$ anchors $\mathbf{r}_i$, $ i = 1, \dots, m$, with $\mathbf{r}_i \in \fatR^2$.
Assume that time-of-arrival measurements provide distance measurements
	\begin{equation}
		d_{i} = {\norm{\mathbf{r}_i - \mathbf{s}}}_2, 
	\end{equation}
from an unknown position $\mathbf{s} \in \fatR^2$. Write a minimal solver that given the two anchor positions $\mathbf{r}_1$ and $\mathbf{r}_2$ and given the two distance measurements $d_1$ and $d_2$ calculates all solutions $\mathbf{s}$ so that
$$
\begin{cases}
d_{1} = {\norm{\mathbf{r}_1 - \mathbf{s}}}_2, \\
d_{2} = {\norm{\mathbf{r}_2 - \mathbf{s}}}_2, 
\end{cases}
$$
\end{example}

\begin{example}[Converting to an eigenvalue problem]
\end{example}

\begin{example}[Uncalibrated relative pose for three views of nine lines]
\end{example}

\begin{example}[Time-of-Arrival node calibration in 2D using 3+3 points]
Time-of-Arrival measurements gives (after multiplication with the speed of sound or speed of light) distance measurements. 
Assume that there are a set of $m=3$ anchors $\mathbf{r}_i$, $ i = 1, \dots, m$, with $\mathbf{r}_i \in \fatR^2$.
Assume that time-of-arrival measurements provide distance measurements
	\begin{equation}
		d_{ij} = {\norm{\mathbf{r}_i - \mathbf{s}_j}}_2, % + \epsilon_{ij}, 
		\label{eq:distance}
	\end{equation}
from $n=3$ unknown positions $\mathbf{s}_j$, $ j = 1, \dots, n$, with $\mathbf{s}_j \in \fatR^2$.

Write a minimal solver that given $9$ distance measurements $d_{ij}, i = 1, \dots, m, j = 1, \dots, n$ calculates all solutions 
$(\mathbf{r}_1, \mathbf{r}_2, \mathbf{r}_3, \mathbf{s}_1, \mathbf{s}_2, \mathbf{s}_3)$ so that
$$
d_{ij} = {\norm{\mathbf{r}_i - \mathbf{s}_j}}_2,    i = 1, \dots, m, j = 1, \dots, n .
$$
Is there enough information to solve this problem? How many solutions are there?
\end{example}

\begin{example}[Time-of-Arrival node calibration in 3D using 4+6 points]
Time-of-Arrival measurements gives (after multiplication with the speed of sound or speed of light) distance measurements. 
Assume that there are a set of $m=4$ anchors $\mathbf{r}_i$, $ i = 1, \dots, m$, with $\mathbf{r}_i \in \fatR^3$.
Assume that time-of-arrival measurements provide distance measurements
	\begin{equation}
		d_{ij} = {\norm{\mathbf{r}_i - \mathbf{s}_j}}_2, % + \epsilon_{ij}, 
		\label{eq:distance}
	\end{equation}
from $n=6$ unknown positions $\mathbf{s}_j$, $ j = 1, \dots, n$, with $\mathbf{s}_j \in \fatR^2$.

Write a minimal solver that given $24$ distance measurements $d_{ij}, i = 1, \dots, m, j = 1, \dots, n$ calculates all solutions 
$(\mathbf{r}_1, \ldots, \mathbf{r}_4, \mathbf{s}_1, \ldots, \mathbf{s}_6)$ so that
$$
d_{ij} = {\norm{\mathbf{r}_i - \mathbf{s}_j}}_2,    i = 1, \dots, m, j = 1, \dots, n .
$$
Is there enough information to solve this problem? How many solutions are there?
\end{example}

\begin{example}[Restricted homographies]
Robot moving in a plane. Unknown orientation with tilt parameters $\psi,\phi$. These are fixed over time. The motion is given by translation $(t_x,t_y)$ and orientation $\theta$. The homography depends on on these five parameters $z = (t_x,t_y,\theta, \psi, \phi)$. $z_0$.
Parametrized homographies $H(z)$. Use elimination to obtain constraints on $H$, e.g.\ $f(H)=0$. The system of polynomial equations

Consider four points in one image $(x_1,y_1), (x_2,y_2), (x_3,y_3), (x_4,y_4)$ and
four points in another images $(u_1,v_1), (u_2,v_2), (u_3,v_3), (u_4,v_4)$. Determine the  $3\times 3$ matrix $H$ representing the homography, such that
$$ \lambda_i \begin{pmatrix} u_i \\ v_i \\ 1 \end{pmatrix} = H \begin{pmatrix} x_i \\ y_i \\ 1 \end{pmatrix} \forall i 
$$
and such that
$f(H)=0$.
What is the variety in this case? Dimension? Degree? Well-determined?
\end{example}

\begin{example}[Optimal trilateration]
Overdetermined problem. More equations than unknowns. Phraze as an optimization problem. Get this problem to be polynomial. 

\end{example}


\section{Open Research Questions}

\begin{example}[The bathing ball problem]
\end{example}


\chapter{Using Eigenvalue Decomposition for polynomial solving}

How are roots calculated in matlab? 

Read \url{https://se.mathworks.com/help/matlab/ref/roots.html} \, .

Consider the polynomial equation 
$$ p(x) = x^3 - 7 x^2 + 14 x - 8 = 0$$
Clearly every solution $x$ to the equation, also fulfills
$$ \begin{cases}
x^3 &=  7 x^2 - 14 x + 8 \\
x^2 &= x^2 \\
x & =x\\
\end{cases}.$$
Rewrite this as a matrix equation
$$
x 
\begin{pmatrix}
x^2\\
x\\
1\\
\end{pmatrix}
=
\underbrace{\begin{pmatrix}
m_{11} & m_{12} & m_{13} \\
m_{21} & m_{22} & m_{23} \\
m_{31} & m_{32} & m_{33} \\
\end{pmatrix}}_{{\bf M}}
\begin{pmatrix}
x^2\\
x\\
1\\
\end{pmatrix}
$$
for some matrix ${\bf M}$. What is the matrix ${\bf M}$?

In the reference page for \verb#roots# in matlab, it is written 'The algorithm simply involves computing the eigenvalues of the companion matrix:'
\begin{verbatim}
A = diag(ones(n-1,1),-1);
A(1,:) = -c(2:n+1)./c(1);
eig(A)
\end{verbatim}
Explain what $c$ is and why the algorithm works

Try 
\begin{verbatim}
c = [1 -7 14 -8];
n = 3;
A = diag(ones(n-1,1),-1);
A(1,:) = -c(2:n+1)./c(1);
[v,d] = eig(A)
\end{verbatim}
Study $v$ and $d$. Explain what they should contain? How can one find the solutions from $d$? How can one find the solutions from $v$? Why is the last row of $v$ not all equal to one? Interpretation?

\begin{example}[Planar trilateration]
Time-of-Arrival measurements gives (after multiplication with the speed of sound or speed of light) distance measurements. 
Assume that there are a set of $m=2$ anchors $\mathbf{r}_i$, $ i = 1, \dots, m$, with $\mathbf{r}_i \in \fatR^2$.
Assume that time-of-arrival measurements provide distance measurements
	\begin{equation}
		d_{i} = {\norm{\mathbf{r}_i - \mathbf{s}}}_2, 
	\end{equation}
from an unknown position $\mathbf{s} \in \fatR^2$. Write a minimal solver that given the two anchor positions $\mathbf{r}_1$ and $\mathbf{r}_2$ and given the two distance measurements $d_1$ and $d_2$ calculates all solutions $\mathbf{s}$ so that
$$
\begin{cases}
d_{1} = {\norm{\mathbf{r}_1 - \mathbf{s}}}_2, \\
d_{2} = {\norm{\mathbf{r}_2 - \mathbf{s}}}_2, 
\end{cases}
$$
\end{example}

\begin{example}[Converting to an eigenvalue problem]
Here we will show how to convert the planar trilateration problem to an eigenvalue problem. This is not the most efficient way to solve this particular problem. However, it is a useful example to illustrate that a system polynomial equation can be solved in a similar technique as for \verb#roots#, i.e.\
\begin{itemize}
\item Mulitply the original equations with a number of monomials.
\item Express the system of equations as $Cm=0$, where $C$ is a matrix of coefficients and $m$ is a vector of monomials. 
\item Use numerical linear algebra to simplify the equations.
\item Rearrange the equations to form an eigenvalue problem $ \lambda {\bf v} = M {\bf v}$. 
\item Calculate eigenvalues and eigenvectors.
\item Extract the solutions from the eigenvalues/eigenvectors. 
\end{itemize}

Read through the example and make sure you understand the different steps. What is similar to roots? Are there any differences?

Assume that the measurements are 
$$
\mathbf{r}_1 = 
\begin{pmatrix}
1 \\
2 
\end{pmatrix} , 
\mathbf{r}_2 = 
\begin{pmatrix}
3 \\
4 
\end{pmatrix}, d_1 = 3, d_2 = 3 .
$$

Parametrize the unknown vector as 
$$ \mathbf{s} = 
\begin{pmatrix}
x \\
y 
\end{pmatrix} .
$$
By squaring the measurements equations we get two polynomial equations in two unknowns. 
$$
\begin{cases}
x^2 - 2 x + y^2 - 4 y - 4 = 0 \\
x^2 - 6 x + y^2 - 8 y + 16 = 0 \\
\end{cases}.
$$
Multiplying the first equation with $1$, $x$ and $y$ and similarily for the second equation gives
$$\begin{cases}
x^2 - 2x + y^2 - 4y - 4 = 0 \\
x^3 - 2x^2 + xy^2 - 4xy -4 x = 0 \\
x^2y - 2xy + y^3 - 4y^2 -4y = 0 \\
x^2 - 6x + y^2 - 8y + 16 = 0 \\
x^3 - 6x^2 + xy^2 - 8xy + 16x = 0 \\
x^2y - 6xy + y^3 - 8y^2 + 16y = 0 \\
\end{cases}.
$$
This can be written as 
$$ C m = \underbrace{\begin{pmatrix}
0 & 0 & 0 & 0 & 1 & 0 & 1 & -2 & -4 & -4 \\
1 & 0 & 1 & 0 & -2 & -4 & 0 & -4 & 0 & 0 \\
0 & 1 & 0 & 1 & 0 & -2 & -4 & 0 & -4 & 0 \\
0 & 0 & 0 & 0 & 1 & 0 & 1 & -6 & -8 & 16 \\
1 & 0 & 1 & 0 & -6 & -8 & 0 & 16 & 0 & 0 \\
0 & 1 & 0 & 1 & 0 & -6 & -8 & 0 & 16 & 0 \\
\end{pmatrix}}_{C} 
\underbrace{\begin{pmatrix}
x^3 \\
x^2y \\
xy^2 \\
y^3 \\
x^2 \\
xy \\
y^2 \\
x \\
y \\
1 \\
\end{pmatrix}}_{m} 
= 0$$

Gaussian Elimination gives six equations 
$$
\begin{pmatrix}
1 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 16 & -81 \\
0 & 1 & 0 & 1 & 0 & 0 & 0 & 0 & -26 & 11 \\
0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 4 & -20 \\
0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 1 & -5.5 \\
0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & -6 & 5.5 \\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 1 & -5 \\
\end{pmatrix}
\underbrace{\begin{pmatrix}
x^3 \\
x^2y \\
xy^2 \\
y^3 \\
x^2 \\
xy \\
y^2 \\
x \\
y \\
1 \\
\end{pmatrix}}_{m} 
= 0 . $$
Two of these size equations are
$$\begin{cases}
xy + y -5.5 = 0 \\
x + y -5 = 0\\
\end{cases}.
$$
Rewriting these as
$$\begin{cases}
xy =  -y +5.5 \\
x =  -y +5 \\
\end{cases}.
$$
gives the following eigenvalue problem
$$
\begin{pmatrix}
xy\\
x\\
\end{pmatrix}
=
x 
\begin{pmatrix}
y\\
1\\
\end{pmatrix}
=
\underbrace{\begin{pmatrix}
-1 & 5.5\\
-1 & 5\\
\end{pmatrix}}_{{\bf M}}
\begin{pmatrix}
y\\
1\\
\end{pmatrix} .
$$
The first eigenvector-eigenvalue pair is 
$\lambda_1 = 0.13$ and $v_1 = \begin{pmatrix} 4.87 \\ 1 \end{pmatrix}$. Thus 
$$\mathbf{s} = \begin{pmatrix} 
0.13\\
4.87
\end{pmatrix}
$$
is one solution to the problem.
The second eigenvector-eigenvalue pair is 
$\lambda_2 = 3.87$ and $v_2 = \begin{pmatrix} 1.13 \\ 1 \end{pmatrix}$. Thus 
$$\mathbf{s} = \begin{pmatrix} 
3.87\\
1.13
\end{pmatrix}
$$
is one solution to the problem.
\end{example}


\chapter{Lecture 10: Fitting, Parameter Estimation}

\section{Examples}
There are many situations in image analysis and computer vision, when the problem of understanding, segmenting or interpreting an image, can be phrased as a problem of estimating parameters to a model. Here are a few examples

\begin{example}[Line detection]
In an image a number of edge points have beed detected using an edge detector. Some of these lie on a line. Find the points that lie on the line and find the parameters of the line. 
\end{example}

\begin{example}[Circle detection]
In an image a number of edge points have beed detected using an edge detector. Some of these lie on a circle. Find the points that lie on the circle and find the parameters of the circle. 
\end{example}

\begin{example}[Epipolar geometry]
In the computer vision guest lecture we will talk about the problem of estimating the relative motion of the camera between two views. 
In two view a number of feature points have beed detected using an interest point detector, e.g.\ SIFT or Harris detector and then matched. Some of these matches fulfill the so called epipolar constraint. Find the point correspondences that actually do match and determine the fundamental matrix. 
\end{example}


In the lecture we use the problem of fitting a line to a set of points as a model problem. The problem is useful, since it is sufficiently complex to capture the main difficulties of many fitting problems, yet simple enough to make the different steps relatively straightforward. 

For line fitting it is common that we assume that a number of measurements of line points (or more generally tokens) are given and that we try to estimate model that fits with these tokens

\begin{table}
\begin{center}
\begin{tabular}{llll}
Case & Token & Model & Constraint \\
\hline
Line  & Edge points $x_i,y_i$ & line parameters $(a,b,c)$ & $ax_i+by_i+c=0$ \\
Circle  & Edge points $x_i,y_i$ & circle parameters $(r,u,v)$ & $(x_i-u)^2+(y_i-v)^2=r^2$ \\
Fundamental matrix  & Point correspondences $x_i,y_i, u_i, v_i$ & Fundamental matrix $F$ & $\begin{pmatrix} x_i & y_i & 1 \end{pmatrix} F \begin{pmatrix} u_i & v_i & 1 \end{pmatrix}^T = 0$ \\
\end{tabular}
\end{center}
\caption{A few model fitting cases with corresponding tokens, models and constraints}
\end{table}

There are (at least) three levels of difficulties to the problem. 

\begin{enumerate}
\item {\bf Parameter Estimation} Assume that we have measurements and know which measurements belong to which objects. How should the parameters of the model be estimated?
\item {\bf Data Association} Assume that we know how many structures are present, and we wish to determine which tokens came from which structure.
For example, we might have a set of edge points, and we need to know the best set of lines fitting these points; this involves (1) determining which points belong together on a line and (2) figuring out what each line is.
Generally, these problems are not independent (because one good way of knowing whether points belong together on a line is checking how well the best fitting line approximates them). 
\item {\bf Model Selection} We would like to know (1) how many structures are present (2) which points are associated with which structure and (3) what the structures are.
For example, given a set of edge points, we might want to return a set of lines that fits them well.
This is, in general, a substantially more difficult problem. The answer depends on the type of model adopted, for example, we could simply pass a line through every pair of edge points ? this gives a set of lines that fit extremely well, but will likely be a poor representation.
\end{enumerate}

\section{Random Sampling and Consensus}

%\bibliographystyle{agsm}
\bibliographystyle{plain}
\bibliography{AlgGeomPolyRefs.bib}

\end{document}
